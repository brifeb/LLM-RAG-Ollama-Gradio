# LLM RAG Ollama Gradio âœ…

Retrieval-Augmented Generation (RAG) with Large Language Model (LLM) using llama-index library and Ollama.

Ollama untuk enabler local local dengan easy setup

Library llama-index sebagai framework RAG, dengan SimpleDirectoryReader, membaca seluruh dokumen dalam folder yang ditentukan

Interface berupa chatbot powered by Gradio

## Install & run Ollama

Kenapa dalam contoh kali ini menggunakan Ollama?

- LLM local
- instalasi yang mudah, dibanding llm cpp
- service tersendiri yang terpisah dari program utama

Installasi: https://ollama.com/download

Run:

1. ollama serve
2. ollama pull llama2

## git pull

### install requirements

> pip install -r requirements.txt

## run app

> python app.py
